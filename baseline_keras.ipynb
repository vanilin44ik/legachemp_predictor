{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b18b0ff",
   "metadata": {},
   "source": [
    "## Обучение многослойного перцептрона для выполнения среды LunarLander на основе фреймворка Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07962600",
   "metadata": {},
   "source": [
    "**Импортирование библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import ops, layers\n",
    "from torch import optim, nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import onnxruntime\n",
    "print(\"Все библиотеки загружены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66e437",
   "metadata": {},
   "source": [
    "**Конфигурация параметров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42       # Псевдо-случайный генератор\n",
    "gamma = 0.99    # Коэффициент дисконтирования прошлых вознаграждений\n",
    "max_steps_per_episode = 1000  # Максимальное кол-во циклов для одного эпизода\n",
    "env = gym.make(\"LunarLander-v3\")  # Создание среды\n",
    "env.reset(seed=seed)    # Инициализация первого кадра\n",
    "eps = np.finfo(np.float32).eps.item()  # Очень маленькое число чтобы избежать деления на ноль\n",
    "print(\"Среда создана и параметры заданы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a2759",
   "metadata": {},
   "source": [
    "**Реализация сети Actor Critic (https://arxiv.org/pdf/1602.01783v2)**\n",
    "\n",
    "Эта сеть выполняет две функции:\n",
    "\n",
    "1. Действующий агент (Actor): Принимает на вход состояние окружающей среды и возвращает значение вероятности\n",
    "для каждого действия в пространстве действий.\n",
    "2. Агент-критик (Critic): Принимает на вход состояние окружающей среды и возвращает\n",
    "оценку общего вознаграждения в будущем.\n",
    "\n",
    "В нашей реализации они совместно используют начальный слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad502cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 8      # Кол-во входов\n",
    "num_actions = 4     # Кол-во действий\n",
    "num_hidden = 32    # Кол-во нейронов в скрытом слое\n",
    "\n",
    "# Конфигурация модели\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "actor_model = keras.Model(inputs=inputs, outputs=action)\n",
    "critic_model = keras.Model(inputs=inputs, outputs=critic)\n",
    "actor_model.summary()\n",
    "critic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a9a0f",
   "metadata": {},
   "source": [
    "**Обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2627d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создаем оптимизатор и функцию потерь для модели\n",
    "actor_optimizer = optim.Adam(actor_model.parameters(), lr=5e-3)\n",
    "critic_optimizer = optim.Adam(critic_model.parameters(), lr=1e-3)\n",
    "huber_loss = keras.losses.Huber()\n",
    "# Сохраняем историю для последующего анализа\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "critic_losses_history = []\n",
    "actor_losses_history = []\n",
    "rewards_history = []\n",
    "running_reward_log = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Запускаем процесс до остановки одним из условий\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "\n",
    "        state = ops.convert_to_tensor(state)\n",
    "        state = ops.expand_dims(state, 0)\n",
    "\n",
    "        # Прогнозирование вероятности действий и оценки будущего\n",
    "        # вознаграждения на основе состояния окружающей среды\n",
    "        action_probs = actor_model(state)\n",
    "        critic_value = critic_model(state)\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "        # Выборка действий из распределения вероятностей действий\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        action_probs_history.append(ops.log(action_probs[0, action]))\n",
    "\n",
    "        # Применяем выбранное действие в нашей среде\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        rewards_history.append(reward)\n",
    "        episode_reward += reward\n",
    "        # Если ракета упала, улетела за пределы видимости или вышло время\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Обновляем вознаграждение за выполнение, чтобы проверить условие для решения\n",
    "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "    running_reward_log.append(running_reward)\n",
    "\n",
    "    # Вычисляем ожидаемое значение от вознаграждений\n",
    "    # - На каждом временном шаге каково было общее вознаграждение, полученное после этого временного шага\n",
    "    # - Вознаграждения в прошлом дисконтируются путем умножения их на гамму\n",
    "    # - Это метки для нашего критика.\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for r in rewards_history[::-1]:\n",
    "        discounted_sum = r + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "\n",
    "    # Нормализуем\n",
    "    returns = np.array(returns)\n",
    "    returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "    returns = returns.tolist()\n",
    "\n",
    "    # Вычисляем значения потерь для обновления нашей сети\n",
    "    history = zip(action_probs_history, critic_value_history, returns)\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    for log_prob, value, ret in history:\n",
    "        # В этот момент истории критик предположил, что в будущем мы получим\n",
    "        # общую награду = `value`. Мы совершили действие с вероятностью\n",
    "        # от `log_prob` и в итоге получили общую награду = `ret`.\n",
    "        # Актёр должен быть обновлен таким образом, чтобы он предсказывал действие, которое приведет к\n",
    "        # высокой награде (по сравнению с оценкой критика) с высокой вероятностью.\n",
    "        diff = ret - value.detach()\n",
    "        actor_loss = -log_prob * diff\n",
    "        actor_losses.append(actor_loss)  # actor loss\n",
    "\n",
    "        # Критик должен быть обновлен таким образом, чтобы он предсказывал более точную оценку\n",
    "        # будущих вознаграждений.\n",
    "        critic_loss = huber_loss(ops.expand_dims(value, 0), ops.expand_dims(ret, 0))\n",
    "        critic_losses.append(critic_loss)\n",
    "\n",
    "    # Обратное распространение\n",
    "    critic_optimizer.zero_grad()\n",
    "    sum(critic_losses).backward()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    sum(actor_losses).backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    critic_losses_history.append(sum(critic_losses).detach().numpy())\n",
    "    actor_losses_history.append(sum(actor_losses).detach().numpy())\n",
    "\n",
    "    # Эпизод закончился поэтому очищаем историю\n",
    "    action_probs_history.clear()\n",
    "    critic_value_history.clear()\n",
    "    rewards_history.clear()\n",
    "\n",
    "    # Логируем прогресс\n",
    "    episode_count += 1\n",
    "    if episode_count % 25 == 0:\n",
    "        template = \"Промежуточная награда: {:.2f} на эпизоде {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    # Условие, при котором задача считается решенной\n",
    "    if running_reward > 250:\n",
    "        print(\"Решили на эпизоде {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb222c5e",
   "metadata": {},
   "source": [
    "**Тестирование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем график по накопленным данным, усредняя кривую по 20 значениям\n",
    "rolling = 20\n",
    "mov_avg_ret = np.convolve(running_reward_log, np.ones(rolling)/rolling, mode='valid')\n",
    "mv_actor = np.convolve(actor_losses_history, np.ones(rolling)/rolling, mode='valid')\n",
    "mv_critic = np.convolve(critic_losses_history, np.ones(rolling)/rolling, mode='valid')\n",
    "# Рисуем графики прогресса\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
    "axs[0,0].plot(mov_avg_ret)\n",
    "axs[0,0].set_title('Награда за эпизод (ср. скользящая)')\n",
    "axs[1,0].plot(mv_critic)\n",
    "axs[1,0].set_title('Critic Loss (ср. скользящая)')\n",
    "axs[1,1].plot(mv_actor)\n",
    "axs[1,1].set_title('Actor Loss (ср. скользящая)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graph.png\")\n",
    "\n",
    "# Проверяем работу нашего агента и записываем видео\n",
    "print(\"Начинаем тестирование!\")\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array', max_episode_steps=500)\n",
    "env = RecordVideo(env, video_folder=\"videos\", name_prefix=\"lander_test\")\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "# Экспортируем в формате onnx\n",
    "actor_model.export(\"model.onnx\", format=\"onnx\")\n",
    "ort_session = onnxruntime.InferenceSession(\"model.onnx\")\n",
    "while not done:\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    output_name = ort_session.get_outputs()[0].name\n",
    "    action = ort_session.run([output_name], {input_name: state[None, :]})[0]\n",
    "    state, reward, terminated, truncated, _ = env.step(np.argmax(action))\n",
    "    done = terminated or truncated\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be452d6",
   "metadata": {},
   "source": [
    "Если все блоки выполнились успешно, поздравляем! Вы получили обученную модель (model.onnx), график обучения (graph.png) и видео с работой вашего агента (videos/lander_test-episode-0.mp4). Видео можно посмотреть локально на компьютере, скачав его через контекстное меню. Теперь, загрузите модель *model.onnx* на сайт http://deepcode.ci.nsu.ru для регистрации в таблице участников. Однако чтобы добиться лучшей точности, вы можете попробовать советы из списка ниже или самостоятельно изменить код обучения.\n",
    "\n",
    "### **Простые улучшения:**\n",
    "- Увеличьте время обучения модели.\n",
    "- Измените расчёт значения для функции потерь, например добавив штраф за долгое выполнение.\n",
    "- Настройте скорость обучения оптимизатора.\n",
    "- \"Усильте\" сеть большим количеством нейронов.\n",
    "- Попробуйте создать разные модели для актёра и критика.\n",
    "\n",
    "### **Сложные улучшения:**\n",
    "- Добавьте градиентный клиппинг чтобы избежать \"взрывов\" градиентов.\n",
    "- Увеличьте энтропию на первых порах обучения.\n",
    "- Векторизуйте среду для обучения параллельных агентов.\n",
    "- Используйте другие имплементации политик."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
